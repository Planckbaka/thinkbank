"""
ThinkBank AI Service - gRPC Server
Implements the AiWorker service defined in ai_service.proto
"""

import asyncio
import sys
from pathlib import Path
from concurrent import futures
from typing import AsyncIterator

import grpc
from loguru import logger

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent))

from core.config import settings
from core.llm import get_llm_service, is_llm_available
from core.embeddings import get_text_embedder, get_image_embedder
from core.vision import get_vision_model

# Import generated protobuf modules (will be generated by protoc)
# from generated import ai_service_pb2, ai_service_pb2_grpc


class AiWorkerServicer:
    """Implementation of the AiWorker gRPC service."""

    def __init__(self):
        self.llm = None
        self.text_embedder = None
        self.image_embedder = None
        self.vision_model = None

    def initialize_models(self):
        """Initialize models lazily."""
        # Initialize embedding models (CPU)
        logger.info("Initializing embedding models...")
        self.text_embedder = get_text_embedder()
        self.image_embedder = get_image_embedder()

        # Initialize vision model (CPU)
        logger.info("Initializing vision model...")
        self.vision_model = get_vision_model()

        # Initialize LLM (GPU) - optional, may fail if no GPU
        if is_llm_available():
            logger.info("Initializing LLM...")
            try:
                self.llm = get_llm_service()
            except Exception as e:
                logger.warning(f"Failed to load LLM: {e}")
        else:
            logger.warning("LLM not available (no GPU or vLLM not installed)")

    async def ProcessAsset(self, request, context):
        """Process an asset (image or document) for AI analysis."""
        from generated import ai_service_pb2

        asset_id = request.asset_id
        file_path = request.file_path
        mime_type = request.mime_type

        logger.info(f"Processing asset: {asset_id}, type: {mime_type}")

        try:
            # This would trigger the async worker
            # For now, return success
            return ai_service_pb2.ProcessStatus(
                success=True,
                error="",
            )
        except Exception as e:
            logger.error(f"Failed to process asset {asset_id}: {e}")
            return ai_service_pb2.ProcessStatus(
                success=False,
                error=str(e),
            )

    async def ChatStream(self, request, context) -> AsyncIterator:
        """Stream chat responses using RAG."""
        from generated import ai_service_pb2

        query = request.query
        history = list(request.history)

        logger.info(f"Chat request: {query[:50]}...")

        if self.llm is None:
            yield ai_service_pb2.ChatResponse(
                chunk="LLM service is not available. Please check GPU configuration.",
                is_final=True,
            )
            return

        try:
            # Generate streaming response
            full_response = ""
            for chunk in self.llm.generate_stream(query):
                full_response += chunk
                yield ai_service_pb2.ChatResponse(
                    chunk=chunk,
                    is_final=False,
                )

            yield ai_service_pb2.ChatResponse(
                chunk="",
                is_final=True,
            )
        except Exception as e:
            logger.error(f"Chat error: {e}")
            yield ai_service_pb2.ChatResponse(
                chunk=f"Error: {str(e)}",
                is_final=True,
            )

    async def GetEmbedding(self, request, context):
        """Generate embeddings for text or image."""
        from generated import ai_service_pb2

        try:
            if request.HasField("text"):
                # Text embedding
                embedding = self.text_embedder.embed_single(request.text)
                return ai_service_pb2.EmbeddingResponse(
                    semantic_vector=embedding,
                    success=True,
                )
            elif request.HasField("image"):
                # Image embedding
                from PIL import Image
                import io
                image = Image.open(io.BytesIO(request.image.data))
                embedding = self.image_embedder.embed_pil(image)
                return ai_service_pb2.EmbeddingResponse(
                    visual_vector=embedding,
                    success=True,
                )
            else:
                return ai_service_pb2.EmbeddingResponse(
                    success=False,
                    error="No content provided",
                )
        except Exception as e:
            logger.error(f"Embedding error: {e}")
            return ai_service_pb2.EmbeddingResponse(
                success=False,
                error=str(e),
            )

    async def VectorSearch(self, request, context):
        """Perform vector similarity search."""
        from generated import ai_service_pb2

        query = request.query
        limit = request.limit or 10
        threshold = request.threshold or 0.7

        logger.info(f"Vector search: {query[:50]}...")

        # TODO: Implement actual vector search with PostgreSQL
        # For now, return empty results
        return ai_service_pb2.SearchResponse(
            results=[],
            total_count=0,
            success=True,
        )


async def serve():
    """Start the gRPC server."""
    # Generate protobuf files first
    await generate_protobuf()

    # Import generated modules
    from generated import ai_service_pb2_grpc, ai_service_pb2

    # Create server
    server = grpc.aio.server(futures.ThreadPoolExecutor(max_workers=10))

    # Create servicer and initialize models
    servicer = AiWorkerServicer()
    servicer.initialize_models()

    # Register service
    ai_service_pb2_grpc.add_AiWorkerServicer_to_server(servicer, server)

    # Start server
    address = f"{settings.grpc_host}:{settings.grpc_port}"
    server.add_insecure_port(address)

    logger.info(f"Starting gRPC server on {address}")
    await server.start()

    try:
        await server.wait_for_termination()
    except KeyboardInterrupt:
        logger.info("Shutting down server...")
        await server.stop(5)


async def generate_protobuf():
    """Generate Python protobuf files from .proto definition."""
    import subprocess

    proto_path = Path(__file__).parent.parent / "go-backend" / "idl" / "ai_service.proto"
    output_dir = Path(__file__).parent / "generated"

    if not proto_path.exists():
        logger.error(f"Proto file not found: {proto_path}")
        raise FileNotFoundError(f"Proto file not found: {proto_path}")

    output_dir.mkdir(exist_ok=True)

    # Generate protobuf and grpc files
    cmd = [
        sys.executable, "-m", "grpc_tools.protoc",
        f"--proto_path={proto_path.parent}",
        f"--python_out={output_dir}",
        f"--grpc_python_out={output_dir}",
        str(proto_path)
    ]

    logger.info(f"Generating protobuf files: {' '.join(cmd)}")
    result = subprocess.run(cmd, capture_output=True, text=True)

    if result.returncode != 0:
        logger.error(f"Protobuf generation failed: {result.stderr}")
        raise RuntimeError(f"Protobuf generation failed: {result.stderr}")

    # Create __init__.py
    (output_dir / "__init__.py").touch()

    logger.info("Protobuf files generated successfully")


if __name__ == "__main__":
    # Configure logging
    logger.remove()
    logger.add(sys.stderr, level="INFO")

    # Run server
    asyncio.run(serve())
